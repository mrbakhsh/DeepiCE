% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/MLP_tuning.R
\name{MLP_tuning}
\alias{MLP_tuning}
\title{Tune Hyperparamters for Multi-layer Perception (MLP)}
\usage{
MLP_tuning(
  train_d,
  train_l,
  nlayers = c(2, 3),
  powerto1 = c(4, 6),
  powerto2 = c(4, 6),
  drate = 0.3,
  optimizer = "rmsprop",
  b_size = 50,
  metrics = "accuracy",
  epochs = 5,
  k = 0.3
)
}
\arguments{
\item{train_d}{A matrix of training data containing numerical features,
generated from \code{\link{build_trainingData}}.}

\item{train_l}{A vector of binary categorical label (0-1),
generated from  \code{\link{build_trainingData}}.}

\item{nlayers}{a vector on integers, describing the number
of hidden layers.}

\item{powerto1}{a vector of integers, describing the number of neurons
in the first hidden layer as defined by two to the power of this value.}

\item{powerto2}{a vector of integers, describing the number of neurons
in the subsequent hidden layer as defined by two to the power of
this value.}

\item{drate}{A vector of numbers, describing the dropout rates range.}

\item{optimizer}{A vector of strings, describing the name of the
optimizer.}

\item{b_size}{A vector of integers, describing
number of samples per gradient update.}

\item{metrics}{List of metrics to be used by the model during k-fold
cross-validation. Defaults to 'accuracy'.
See \code{\link[keras]{compile.keras.engine.training.Model}} for
more details.}

\item{epochs}{Number of epochs to train the model.}

\item{k}{Float between 0 and 1. Fraction of the training data to be used
as validation data.}
}
\value{
A data.frame containing model performance across different
combination of parameters.
}
\description{
This function tune different combinations of hyperparameters
for Multi-layer Perception (MLP) model.
}
\details{
MLP_tuning
}
\examples{
#' ## first build training set
#load the co-elution data
data("HelaCE")
HelaCE <- HelaCE[1:100,]
#load the reference data for training
data("refcpx")
# concatenate the profile
m_combined <- getPPI(HelaCE)
# build training data
t_data <- build_trainingData(m_combined, refcpx)
## now run the tuning
library("tensorflow")
set_random_seed(4)
tuning_result <-
MLP_tuning(t_data$train_d,
t_data$train_l,
nlayers = 2,
powerto1 = c(5,6), # tune number of nodes in hidden layer 1
metrics = "accuracy",
epochs = 5, k = 0.3)
}
\author{
Matineh Rahmatbakhsh, \email{matinerb.94@gmail.com}
}
